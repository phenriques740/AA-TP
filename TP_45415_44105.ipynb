{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, time, datetime\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV , RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA , TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os ficheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target\n",
    "\n",
    "with open('tfidf_dump.p', 'rb') as f:\n",
    "    global tfidf\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos analisar a quantidade de reviews de cada uma das possíveis reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rating, value=np.unique(y,return_counts=True)\n",
    "plt.plot(rating,value,'*r')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o lancaster, porque teve os melhores resultados e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'lancaster', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, LancasterStemmer())\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    Doc = ' '.join([stemFunc.stem(w) for w in Doc.split()])\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='lancaster', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs, preProcess = False, stemmer='lancaster', decode=False, targets=None, normalize=True):\n",
    "    if(preProcess):\n",
    "        Docs = preProcessDocs(Docs, stemmer=stemmer, decode=decode)\n",
    "    \n",
    "    global tfidf\n",
    "    try:\n",
    "        tfidf ## ver se tfidf já está definido\n",
    "    except:\n",
    "        tfidf = pickle.load(open('tfidf_dump.p','rb'))\n",
    "\n",
    "        \n",
    "    X = tfidf.transform(Docs)\n",
    "    if not normalize:\n",
    "        return X\n",
    "    ##return Normalizer().fit_transform(X, targets)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação Binária\n",
    "\n",
    "Converter da escala de 0 a 10, para negativos/positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_boolean = [0 if val<5 else 1 for val in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steemers\n",
    "\n",
    "O stemmer reduz uma palavra á sua raiz, ou seja, remove plurais, conjugacao de verbos, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26773\n",
      "0.9455\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='porter')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26394\n",
      "0.945375\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='snowball')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 21883\n",
      "0.9394\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='lancaster')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P 28315.486105602708\n",
      "S 27919.818056804357\n",
      "L 23294.036245575753\n"
     ]
    }
   ],
   "source": [
    "print('P' ,26773/0.945525)\n",
    "print('S' ,26394/0.94535)\n",
    "print('L' ,21883/0.939425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel ver que o lancaster é o que reduz ao maximo a quantidade de tokens, alem disso vamos optar por utilizar o Lancaster pois precisava de uma quantidade de tokens menores para atingir os teoricos 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar modelos lineares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs_dump.p', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)\n",
    "with open('docs_dump.p', 'wb') as f:\n",
    "    pickle.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 15:16:13\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 56.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 28.11s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at 16:17:27\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory(prefix=\"sklearn_graph_cache_\") as tmpdir:\n",
    "    global pipeline, grid_param, grid_search\n",
    "    \n",
    "    pipeline =Pipeline([\n",
    "        ('tfidf' , TfidfVectorizer()),\n",
    "        ('clf' , LogisticRegressionCV(max_iter = 1000, tol=1e-3))\n",
    "    ], memory=tmpdir)\n",
    "\n",
    "    grid_param ={\n",
    "        'tfidf__strip_accents' :[None, 'unicode'],\n",
    "        #'tfidf__stop_words' : [None, 'english'],\n",
    "        'tfidf__token_pattern' : [r'\\b\\w{3,}\\b', r'\\b[a-zA-Z]{3,}\\b'],\n",
    "        'tfidf__min_df' : np.arange(3, 5, 1),\n",
    "        'tfidf__max_df' : [0.25, 0.5, 0.75],\n",
    "        #'tfidf__min_df' : [3,4,5],\n",
    "        #'tfidf__min_df' : [3],\n",
    "        #'tfidf__ngram_range' : [(i,j) for i in range(1,5) for j in range(1,5)],\n",
    "        'tfidf__ngram_range' : [(1,1), (1,2), (1,3) , (2,2)] ,\n",
    "        'tfidf__norm' : ['l1', 'l2'],\n",
    "        'tfidf__max_features' : [None, 15000,20000,25000,30000],\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "        #'clf__C' : np.linspace(0.1,10,100),\n",
    "        'clf__Cs' : [1,3,10,30,100], \n",
    "        #'clf__C' : [3.3],\n",
    "        'clf__solver' : ['sag', 'saga'],\n",
    "        #'clf__tol' : (1e-3, 1e-4, 1e-5)\n",
    "    }\n",
    "    t0 =time.localtime()\n",
    "    print('Started at'  , time.strftime(\"%H:%M:%S\", t0))\n",
    "    ## Se tiveres mais memoria que eu(8Gb), aumenta o pre_dispatch para um valor maior\n",
    "    grid_search = RandomizedSearchCV(pipeline, grid_param, cv = 3, n_jobs=-1, verbose=3,\\\n",
    "                                     pre_dispatch=8,n_iter = 30).fit(X, y_boolean)\n",
    "\n",
    "    t1 = time.localtime()\n",
    "    print('Done at' , time.strftime(\"%H:%M:%S\", t1))\n",
    "with open('dump.p', 'wb') as f:\n",
    "    pickle.dump({'in' : grid_param ,\n",
    "                 'out': grid_search}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time\n",
      "\t[690.02867977  37.32141058 369.89045056 137.35754371 456.18932152\n",
      " 184.8494792  236.27336701 402.69934948 289.6661249  756.45097788\n",
      " 268.18874661 187.31101513  66.68613791 175.29305712  90.3015159 ]\n",
      "std_fit_time\n",
      "\t[ 5.4261045   4.62307497 50.23601666  1.57604928  8.96949775  3.51713334\n",
      "  4.20336944  3.99793306 11.44582785 18.09002713  7.72037476  1.22601322\n",
      "  0.6180658   4.00822898 15.02676939]\n",
      "mean_score_time\n",
      "\t[7.50773851 5.12932785 6.34752345 4.35355218 5.90589515 6.3094004\n",
      " 4.53880405 6.44200897 3.91615399 5.21079381 9.53086813 6.47203175\n",
      " 6.27580039 4.19172374 2.21245686]\n",
      "std_score_time\n",
      "\t[0.21851615 0.71846843 0.46596361 0.10303288 0.20755637 0.20009386\n",
      " 0.14472093 0.11510817 0.48230765 0.36749729 0.60210629 0.21899587\n",
      " 0.3289914  0.46973708 0.67657892]\n",
      "param_tfidf__token_pattern\n",
      "\t['\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b'\n",
      " '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b']\n",
      "param_tfidf__strip_accents\n",
      "\t[None None 'unicode' 'unicode' 'unicode' 'unicode' 'unicode' None\n",
      " 'unicode' None None 'unicode' None None None]\n",
      "param_tfidf__norm\n",
      "\t['l2' 'l2' 'l2' 'l1' 'l2' 'l1' 'l1' 'l1' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2'\n",
      " 'l2']\n",
      "param_tfidf__ngram_range\n",
      "\t[(1, 2) (1, 1) (1, 2) (2, 2) (1, 2) (1, 2) (2, 2) (1, 2) (1, 1) (2, 2)\n",
      " (1, 3) (1, 2) (1, 2) (2, 2) (1, 1)]\n",
      "param_tfidf__min_df\n",
      "\t[4 3 3 4 4 4 3 3 4 4 3 4 4 3 4]\n",
      "param_tfidf__max_features\n",
      "\t[None 15000 20000 30000 15000 15000 20000 25000 None None 20000 15000\n",
      " 20000 15000 20000]\n",
      "param_tfidf__max_df\n",
      "\t[0.75 0.25 0.25 0.25 0.25 0.5 0.75 0.75 0.5 0.75 0.25 0.5 0.25 0.25 0.5]\n",
      "param_norm__norm\n",
      "\t['l1' 'l2' 'l1' 'l2' 'l2' 'max' 'max' 'l1' 'l1' 'l2' 'l1' 'max' 'l2' 'l2'\n",
      " 'max']\n",
      "param_clf__solver\n",
      "\t['saga' 'sag' 'saga' 'saga' 'saga' 'saga' 'sag' 'saga' 'sag' 'sag' 'sag'\n",
      " 'saga' 'sag' 'saga' 'saga']\n",
      "param_clf__Cs\n",
      "\t[3 1 30 10 100 3 100 30 100 100 3 3 1 30 3]\n",
      "params\n",
      "\t[{'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.75, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 3, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 1}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 30000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 10}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.75, 'norm__norm': 'max', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 25000, 'tfidf__max_df': 0.75, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.5, 'norm__norm': 'l1', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.75, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l1', 'clf__solver': 'sag', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 1}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 4, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}]\n",
      "split0_test_score\n",
      "\t[0.89485526 0.50524974 0.88870556 0.87288136 0.88615569 0.88255587\n",
      " 0.86703165 0.89223039 0.87490625 0.88233088 0.88750562 0.88225589\n",
      " 0.50524974 0.86440678 0.8720564 ]\n",
      "split1_test_score\n",
      "\t[0.9038476  0.50521263 0.90152254 0.87489687 0.89739743 0.89289732\n",
      " 0.8679967  0.90197255 0.88779719 0.8841221  0.89859746 0.89274732\n",
      " 0.50521263 0.86462162 0.88712218]\n",
      "split2_test_score\n",
      "\t[0.89949749 0.50528763 0.8961974  0.87924698 0.89552239 0.88839721\n",
      " 0.87114678 0.89724743 0.879997   0.88824721 0.89522238 0.88824721\n",
      " 0.50528763 0.8680717  0.880147  ]\n",
      "mean_test_score\n",
      "\t[0.89940011 0.50525    0.89547517 0.87567507 0.89302517 0.88795013\n",
      " 0.86872504 0.89715012 0.88090015 0.88490006 0.89377516 0.88775014\n",
      " 0.50525    0.86570003 0.87977519]\n",
      "std_test_score\n",
      "\t[3.67175264e-03 3.06199498e-05 5.25737068e-03 2.65637467e-03\n",
      " 4.91740188e-03 4.23369856e-03 1.75716367e-03 3.97781566e-03\n",
      " 5.30131084e-03 2.47718070e-03 4.64241837e-03 4.29750638e-03\n",
      " 3.06199498e-05 1.67931546e-03 6.15619584e-03]\n",
      "rank_test_score\n",
      "\t[ 1 14  3 11  5  6 12  2  9  8  4  7 14 13 10]\n",
      "0\n",
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "             print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "pretty(grid_search.cv_results_, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_index_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em vez de correr o codigo em cima, podemos fazer import do ficheiro dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "with open('dump.p' ,'rb') as f:\n",
    "    global grid_param, grid_search\n",
    "    temp = pickle.load(f)\n",
    "    grid_param = temp['in']\n",
    "    grid_search = temp['out']\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TFIDF optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf_dump.p', 'wb') as f:\n",
    "    tfidf = TfidfVectorizer(max_df=0.75 , min_df=4,ngram_range=(1, 2), norm='l2', token_pattern=r'\\b\\w{3,}\\b',).fit(X)\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao booleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = text2vector(X, targets=y_boolean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector_dump.p', 'wb') as f:\n",
    "    pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector_dump.p', 'rb') as f:\n",
    "    vector = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{1: array([[0.50525 , 0.888   , 0.89675 ],\n",
      "       [0.50525 , 0.893625, 0.90275 ],\n",
      "       [0.50525 , 0.898875, 0.904875],\n",
      "       [0.50525 , 0.903875, 0.913375],\n",
      "       [0.50525 , 0.895625, 0.909125]])}\n"
     ]
    }
   ],
   "source": [
    "##tfidf = TfidfVectorizer(min_df = 3, token_pattern=r'\\b\\w{3,}\\b', norm = 'l2' , ngram_range=(1,2)).fit(Docs)\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(vector,y_boolean)\n",
    "print(dl.score(vector, y_boolean))\n",
    "print(dl.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9085\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vector, y_boolean, test_size=1/4)\n",
    "\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(x_train, y_train)\n",
    "print(dl.score(x_train, y_train))\n",
    "print(dl.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803\n",
      "0.4529\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vector, y, test_size=1/4)\n",
    "\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(x_train, y_train)\n",
    "print(dl.score(x_train, y_train))\n",
    "print(dl.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC (multiclasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y1,y2=train_test_split(vector,y,test_size=1/5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=LinearSVC(dual=False,C=1).fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye=svm.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho y:  (40000,)\n",
      "tamanho ye:  (8000,)\n",
      "N erros(teste): 4813\n"
     ]
    }
   ],
   "source": [
    "print(\"tamanho y: \",y.shape)\n",
    "print(\"tamanho ye: \",ye.shape)\n",
    "print('N erros(teste):',np.sum(y2!=ye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score train:  0.85334375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score train: \", svm.score(X1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score test:  0.398375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score test: \", svm.score(X2,y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas condições de teste não são as mais adecuadas para o conjunto de dados pois como podemos visualizar no teste acima, os valores de score para o conjunto de train e teste são bastante díspares,sendo o de train bastante bom ao contrário do de test que não é muito bom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC (booleana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y1,y2=train_test_split(vector,y_boolean,test_size=1/5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=LinearSVC(dual=False,C=1).fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye=svm.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho y:  (40000,)\n",
      "tamanho ye:  (8000,)\n",
      "N erros(teste): 716\n"
     ]
    }
   ],
   "source": [
    "print(\"tamanho y: \",y.shape)\n",
    "print(\"tamanho ye: \",ye.shape)\n",
    "print('N erros(teste):',np.sum(y2!=ye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score test:  0.9993125\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score test: \", svm.score(X1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score test:  0.9105\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score test: \", svm.score(X2,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposição em componentes principais\n",
    "\n",
    "Temos 4000 documentos, com 240000 tokens, logo para reduzir a complixidade usamos LDA. A LDA remove os componentes com pouco relevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 210238)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não repetir, demora muito, tipo umas três horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=100).fit(vector, y_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05593491422460291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_dump.p' ,'wb') as f:\n",
    "    pickle.dump(pca , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Componentes         10 ||  Variance  0.0145409012\n",
      "Num Componentes         20 ||  Variance  0.0212406198\n",
      "Num Componentes         30 ||  Variance  0.0263198459\n",
      "Num Componentes         50 ||  Variance  0.0346090723\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 176. MiB for an array with shape (210238, 110) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3135de0bae4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_comps\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_comps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Num Componentes {n_comps : >10} ||  Variance {pca.explained_variance_ratio_.sum() : .10f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    181\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0;32m    182\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                                           random_state=random_state)\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown algorithm %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mpower_iteration_normalizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpower_iteration_normalizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         random_state=random_state)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;31m# Sample the range of A using by linear projection of Q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;31m# Extract an orthonormal basis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'economic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\linalg\\decomp_qr.py\u001b[0m in \u001b[0;36mqr\u001b[1;34m(a, overwrite_a, lwork, mode, pivoting, check_finite)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mgeqrf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'geqrf'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         qr, tau = safecall(geqrf, \"geqrf\", a1, lwork=lwork,\n\u001b[1;32m--> 141\u001b[1;33m                            overwrite_a=overwrite_a)\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'economic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\linalg\\decomp_qr.py\u001b[0m in \u001b[0;36msafecall\u001b[1;34m(f, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lwork'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         raise ValueError(\"illegal value in %dth argument of internal %s\"\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 176. MiB for an array with shape (210238, 110) and data type float64"
     ]
    }
   ],
   "source": [
    "list = [10,20,30,50,100,200,300,500]\n",
    "\n",
    "for n_comps in list:\n",
    "    pca = TruncatedSVD(n_components=n_comps, n_iter=2).fit(vector)\n",
    "    print(f'Num Componentes {n_comps : >10} ||  Variance {pca.explained_variance_ratio_.sum() : .10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_dump.p' , 'rb') as f:\n",
    "    global pca\n",
    "    pca = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = pca.transform(vector)\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.868175\n",
      "{1: array([[0.50525 , 0.86275 , 0.86425 ],\n",
      "       [0.50525 , 0.864875, 0.8645  ],\n",
      "       [0.50525 , 0.86175 , 0.86325 ],\n",
      "       [0.50525 , 0.872625, 0.871625],\n",
      "       [0.50525 , 0.8625  , 0.867875]])}\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegressionCV(Cs=3, max_iter=1000, multi_class='auto', n_jobs=-1, penalty='l2', solver='saga', \n",
    "                     tol=0.001).fit(vec, y_boolean)\n",
    "print(dl.score(vec, y_boolean))\n",
    "print(dl.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{1: array([[0.50525 , 0.887875, 0.896375],\n",
      "       [0.50525 , 0.89375 , 0.9025  ],\n",
      "       [0.50525 , 0.898875, 0.904625],\n",
      "       [0.50525 , 0.903875, 0.914   ],\n",
      "       [0.50525 , 0.895625, 0.909125]])}\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegressionCV(Cs=3, max_iter=1000, multi_class='auto', n_jobs=-1, penalty='l2', solver='saga', \n",
    "                     tol=0.001).fit(vector, y_boolean)\n",
    "print(dl.score(vector, y_boolean))\n",
    "print(dl.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 210238 features per sample; expecting 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6f298b6611eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_boolean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2086\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    168\u001b[0m                           stacklevel=2)\n\u001b[0;32m    169\u001b[0m         return self._score(partial(_cached_call, None), estimator, X, y_true,\n\u001b[1;32m--> 170\u001b[1;33m                            sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_factory_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \"\"\"\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \"\"\"\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 287\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 210238 features per sample; expecting 100"
     ]
    }
   ],
   "source": [
    "print(dl.score(vector,y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Escolher o classificador\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "\n",
    "### Pipeline\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "### Clustering\n",
    "- https://medium.com/hanman/data-clustering-what-type-of-movies-are-in-the-imdb-top-250-7ef59372a93b\n",
    "\n",
    "### Decomposição PCA/LSA\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD\n",
    "- https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf\n",
    "- https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
