{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, time, datetime\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV , RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA , TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os ficheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o lancaster, porque teve os melhores resultados e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'lancaster', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, LancasterStemmer())\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    Doc = ' '.join([stemFunc.stem(w) for w in Doc.split()])\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='lancaster', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs, preProcess = False, stemmer='lancaster', decode=False):\n",
    "    if(preProcess):\n",
    "        Docs = preProcessDocs(Docs, stemmer=stemmer, decode=decode)\n",
    "    global tfidf\n",
    "    print(tfidf)\n",
    "    if(tfidf is None):\n",
    "        tfidf = pickle.load(open('tfidf_dump.p','rb'))\n",
    "        #tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', ngram_range=(1,2),\\\n",
    "        #                        norm = 'l2').fit(Docs)\n",
    "    X = tfidf.transform(Docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação Binária\n",
    "\n",
    "Converter da escala de 0 a 10, para negativos/positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_boolean = [0 if val<5 else 1 for val in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steemers\n",
    "\n",
    "O stemmer reduz uma palavra á sua raiz, ou seja, remove plurais, conjugacao de verbos, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26773\n",
      "0.9455\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='porter')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26394\n",
      "0.945375\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='snowball')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 21883\n",
      "0.9394\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='lancaster')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P 28315.486105602708\n",
      "S 27919.818056804357\n",
      "L 23294.036245575753\n"
     ]
    }
   ],
   "source": [
    "print('P' ,26773/0.945525)\n",
    "print('S' ,26394/0.94535)\n",
    "print('L' ,21883/0.939425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel ver que o lancaster é o que reduz ao maximo a quantidade de tokens, alem disso vamos optar por utilizar o Lancaster pois precisava de uma quantidade de tokens menores para atingir os teoricos 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar modelos lineares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done fitting\n"
     ]
    }
   ],
   "source": [
    "X=preProcessDocs(Docs)\n",
    "with open('tfidf_dump', 'wb') as f:\n",
    "    tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', \\\n",
    "                            ngram_range=(1,2), norm = 'l2').fit(X)\n",
    "    print('done fitting')\n",
    "    pickle.dump(tfidf, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289621"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 15:16:13\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 56.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 28.11s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory(prefix=\"sklearn_graph_cache_\") as tmpdir:\n",
    "    pipeline =Pipeline([\n",
    "        ('tfidf' , TfidfVectorizer()),\n",
    "        ('norm' , Normalizer()),\n",
    "        ('clf' , LogisticRegressionCV(max_iter = 1000, tol=1e-3))\n",
    "    ], memory=tmpdir)\n",
    "\n",
    "    grid_param ={\n",
    "        'tfidf__strip_accents' :[None, 'unicode'],\n",
    "        #'tfidf__stop_words' : [None, 'english'],\n",
    "        'tfidf__token_pattern' : [r'\\b\\w{3,}\\b', r'\\b[a-zA-Z]{3,}\\b'],\n",
    "        'tfidf__min_df' : np.arange(3, 5, 1),\n",
    "        'tfidf__max_df' : [0.25, 0.5, 0.75],\n",
    "        #'tfidf__min_df' : [3,4,5],\n",
    "        #'tfidf__min_df' : [3],\n",
    "        #'tfidf__ngram_range' : [(i,j) for i in range(1,5) for j in range(1,5)],\n",
    "        'tfidf__ngram_range' : [(1,1), (1,2), (1,3) , (2,2)] ,\n",
    "        'tfidf__norm' : ['l1', 'l2'],\n",
    "        'tfidf__max_features' : [None, 15000,20000,25000,30000],\n",
    "\n",
    "        'norm__norm' : ['l1', 'l2', 'max'],\n",
    "\n",
    "\n",
    "        #'clf__C' : np.linspace(0.1,10,100),\n",
    "        'clf__Cs' : [1,3,10,30,100], \n",
    "        #'clf__C' : [3.3],\n",
    "        'clf__solver' : ['sag', 'saga'],\n",
    "        #'clf__tol' : (1e-3, 1e-4, 1e-5)\n",
    "    }\n",
    "    t0 =time.localtime()\n",
    "    print('Started at'  , time.strftime(\"%H:%M:%S\", t0))\n",
    "    ## Se tiveres mais memoria que eu(8Gb), aumenta o pre_dispatch para um valor maior\n",
    "    grid_search = RandomizedSearchCV(pipeline, grid_param, cv = 3, n_jobs=-1, verbose=3,\\\n",
    "                                     pre_dispatch=8,n_iter = 15).fit(X, y_boolean)\n",
    "\n",
    "    t1 = time.localtime()\n",
    "    print('Done at' , time.strftime(\"%H:%M:%S\", t1))\n",
    "with open('dump.p', 'wb') as f:\n",
    "    pickle.dump({'in' : grid_param ,\n",
    "                 'out': grid_search}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time\n",
      "\t[793.77378281 203.92118669 135.58186412  67.24587297 217.12415775]\n",
      "std_fit_time\n",
      "\t[16.46313034  2.55856885  9.56936036  0.57878529  1.26411078]\n",
      "mean_score_time\n",
      "\t[5.15522528 4.77492126 5.2886459  4.90518769 2.57232587]\n",
      "std_score_time\n",
      "\t[0.13301035 0.09226935 0.22727117 0.4655577  0.81811887]\n",
      "param_tfidf__token_pattern\n",
      "\t['\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b'\n",
      " '\\\\b\\\\w{3,}\\\\b']\n",
      "param_tfidf__strip_accents\n",
      "\t[None None None None None]\n",
      "param_tfidf__norm\n",
      "\t['l2' 'l2' 'l2' 'l2' 'l1']\n",
      "param_tfidf__ngram_range\n",
      "\t[(2, 2) (2, 2) (2, 2) (2, 2) (1, 1)]\n",
      "param_tfidf__min_df\n",
      "\t[4 4 4 4 4]\n",
      "param_tfidf__max_features\n",
      "\t[None 30000 None 20000 15000]\n",
      "param_tfidf__max_df\n",
      "\t[0.25 0.25 0.25 0.75 0.25]\n",
      "param_norm__norm\n",
      "\t['l2' 'l2' 'l1' 'l2' 'max']\n",
      "param_clf__solver\n",
      "\t['saga' 'saga' 'sag' 'sag' 'sag']\n",
      "param_clf__Cs\n",
      "\t[100 30 1 3 100]\n",
      "params\n",
      "\t[{'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 30000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.25, 'norm__norm': 'l1', 'clf__solver': 'sag', 'clf__Cs': 1}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.75, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'max', 'clf__solver': 'sag', 'clf__Cs': 100}]\n",
      "split0_test_score\n",
      "\t[0.88173091 0.8719814  0.50524974 0.86538173 0.87235638]\n",
      "split1_test_score\n",
      "\t[0.88494712 0.87474687 0.50521263 0.86672167 0.8839721 ]\n",
      "split2_test_score\n",
      "\t[0.88667217 0.87939698 0.50528763 0.8721218  0.87804695]\n",
      "mean_test_score\n",
      "\t[0.88445007 0.87537508 0.50525    0.86807507 0.87812514]\n",
      "std_test_score\n",
      "\t[2.04764815e-03 3.05981624e-03 3.06199498e-05 2.91329256e-03\n",
      " 4.74241898e-03]\n",
      "rank_test_score\n",
      "\t[1 3 5 4 2]\n",
      "0\n",
      "Pipeline(memory=None,\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.25, max_features=None,\n",
      "                                 min_df=4, ngram_range=(2, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern=...\n",
      "                                 use_idf=True, vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l2')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=100, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "             print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "pretty(grid_search.cv_results_, 0)\n",
    "print(grid_search.best_index_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em vez de correr o codigo em cima, podemos fazer import do ficheiro dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('dump.p' ,'rb') as f:\n",
    "    global grid_param, grid_search\n",
    "    temp = pickle.load(f)\n",
    "    grid_param = temp['in']\n",
    "    grid_search = temp['out']\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)\n",
    "tfidf = TfidfVectorizer(min_df = 3, token_pattern=r'\\b\\w{3,}\\b', norm = 'l2' , ngram_range=(1,2)).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21883"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao booleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91805\n",
      "{1: array([[0.50525 , 0.880375, 0.839375],\n",
      "       [0.50525 , 0.888375, 0.850625],\n",
      "       [0.50525 , 0.886625, 0.85    ],\n",
      "       [0.50525 , 0.888375, 0.84275 ],\n",
      "       [0.50525 , 0.88625 , 0.854   ]])}\n"
     ]
    }
   ],
   "source": [
    "##tfidf = TfidfVectorizer(min_df = 3, token_pattern=r'\\b\\w{3,}\\b', norm = 'l2' , ngram_range=(1,2)).fit(Docs)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5).fit(vector,y_boolean)\n",
    "print(dl.score(vector, y_boolean))\n",
    "print(dl.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9181\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5).fit(vector,y_boolean)\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.623375\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5).fit(vector, y)\n",
    "print(dl.score(vector, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y1,y2=train_test_split(vector,y,test_size=1/5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=LinearSVC(dual=False,C=1).fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye=svm.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho y:  (40000,)\n",
      "tamanho ye:  (8000,)\n",
      "N erros(teste): 4813\n"
     ]
    }
   ],
   "source": [
    "print(\"tamanho y: \",y.shape)\n",
    "print(\"tamanho ye: \",ye.shape)\n",
    "print('N erros(teste):',np.sum(y2!=ye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score train:  0.85334375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score train: \", svm.score(X1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score test:  0.398375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score test: \", svm.score(X2,y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas condições de teste não são as mais adecuadas para o conjunto de dados pois como podemos visualizar no teste acima, os valores de score para o conjunto de train e teste são bastante díspares,sendo o de train bastante bom ao contrário do de test que não é muito bom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposição em componentes principais\n",
    "\n",
    "Temos 4000 documentos, com 24000 tokes, logo para reduzir a complixidade usamos LDA. A LDA remove os componentes com pouco relevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 321899)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 270. MiB for an array with shape (321899, 110) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-be193f94f73f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\decomposition\\_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    181\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0;32m    182\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m                                           random_state=random_state)\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown algorithm %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mpower_iteration_normalizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpower_iteration_normalizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         random_state=random_state)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LU'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'QR'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    558\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    559\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         result = np.zeros((M, n_vecs),\n\u001b[1;32m--> 485\u001b[1;33m                           dtype=upcast_char(self.dtype.char, other.dtype.char))\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 270. MiB for an array with shape (321899, 110) and data type float64"
     ]
    }
   ],
   "source": [
    "pca = TruncatedSVD(n_components=100).fit(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_PCA=[np.round(a, 6) for a in pca.explained_variance_ratio_ if a >0.00001]\n",
    "sum(top_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Escolher o classificador\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "\n",
    "### Pipeline\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "### Clustering\n",
    "- https://medium.com/hanman/data-clustering-what-type-of-movies-are-in-the-imdb-top-250-7ef59372a93b\n",
    "\n",
    "### Decomposição PCA/LSA\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD\n",
    "- https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf\n",
    "- https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
