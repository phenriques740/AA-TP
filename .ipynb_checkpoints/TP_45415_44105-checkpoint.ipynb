{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, time, datetime\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV , RandomizedSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA , TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar os ficheiros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target\n",
    "\n",
    "with open('tfidf_dump.p', 'rb') as f:\n",
    "    global tfidf\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o lancaster, porque teve os melhores resultados e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'lancaster', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, LancasterStemmer())\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    Doc = ' '.join([stemFunc.stem(w) for w in Doc.split()])\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='lancaster', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs, preProcess = False, stemmer='lancaster', decode=False, targets=None, normalize=True):\n",
    "    if(preProcess):\n",
    "        Docs = preProcessDocs(Docs, stemmer=stemmer, decode=decode)\n",
    "    \n",
    "    global tfidf\n",
    "    try:\n",
    "        tfidf ## ver se tfidf já está definido\n",
    "    except:\n",
    "        tfidf = pickle.load(open('tfidf_dump.p','rb'))\n",
    "\n",
    "        \n",
    "    X = tfidf.transform(Docs)\n",
    "    if not normalize:\n",
    "        return X\n",
    "    return Normalizer().fit_transform(X, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x210238 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3374453 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2vector(Docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação Binária\n",
    "\n",
    "Converter da escala de 0 a 10, para negativos/positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_boolean = [0 if val<5 else 1 for val in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steemers\n",
    "\n",
    "O stemmer reduz uma palavra á sua raiz, ou seja, remove plurais, conjugacao de verbos, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26773\n",
      "0.9455\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='porter')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 26394\n",
      "0.945375\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='snowball')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 21883\n",
      "0.9394\n"
     ]
    }
   ],
   "source": [
    "X = preProcessDocs(Docs, stemmer='lancaster')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tfidf.get_feature_names()))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P 28315.486105602708\n",
      "S 27919.818056804357\n",
      "L 23294.036245575753\n"
     ]
    }
   ],
   "source": [
    "print('P' ,26773/0.945525)\n",
    "print('S' ,26394/0.94535)\n",
    "print('L' ,21883/0.939425)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel ver que o lancaster é o que reduz ao maximo a quantidade de tokens, alem disso vamos optar por utilizar o Lancaster pois precisava de uma quantidade de tokens menores para atingir os teoricos 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar modelos lineares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 15:16:13\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 33.9min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 56.1min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:315: UserWarning: Persisting input arguments took 28.11s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  **fit_params_steps[name])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at 16:17:27\n"
     ]
    }
   ],
   "source": [
    "with TemporaryDirectory(prefix=\"sklearn_graph_cache_\") as tmpdir:\n",
    "    global pipeline, grid_param, grid_search\n",
    "    \n",
    "    pipeline =Pipeline([\n",
    "        ('tfidf' , TfidfVectorizer()),\n",
    "        ('norm' , Normalizer()),\n",
    "        ('clf' , LogisticRegressionCV(max_iter = 1000, tol=1e-3))\n",
    "    ], memory=tmpdir)\n",
    "\n",
    "    grid_param ={\n",
    "        'tfidf__strip_accents' :[None, 'unicode'],\n",
    "        #'tfidf__stop_words' : [None, 'english'],\n",
    "        'tfidf__token_pattern' : [r'\\b\\w{3,}\\b', r'\\b[a-zA-Z]{3,}\\b'],\n",
    "        'tfidf__min_df' : np.arange(3, 5, 1),\n",
    "        'tfidf__max_df' : [0.25, 0.5, 0.75],\n",
    "        #'tfidf__min_df' : [3,4,5],\n",
    "        #'tfidf__min_df' : [3],\n",
    "        #'tfidf__ngram_range' : [(i,j) for i in range(1,5) for j in range(1,5)],\n",
    "        'tfidf__ngram_range' : [(1,1), (1,2), (1,3) , (2,2)] ,\n",
    "        'tfidf__norm' : ['l1', 'l2'],\n",
    "        'tfidf__max_features' : [None, 15000,20000,25000,30000],\n",
    "\n",
    "        'norm__norm' : ['l1', 'l2', 'max'],\n",
    "\n",
    "\n",
    "        #'clf__C' : np.linspace(0.1,10,100),\n",
    "        'clf__Cs' : [1,3,10,30,100], \n",
    "        #'clf__C' : [3.3],\n",
    "        'clf__solver' : ['sag', 'saga'],\n",
    "        #'clf__tol' : (1e-3, 1e-4, 1e-5)\n",
    "    }\n",
    "    t0 =time.localtime()\n",
    "    print('Started at'  , time.strftime(\"%H:%M:%S\", t0))\n",
    "    ## Se tiveres mais memoria que eu(8Gb), aumenta o pre_dispatch para um valor maior\n",
    "    grid_search = RandomizedSearchCV(pipeline, grid_param, cv = 3, n_jobs=-1, verbose=3,\\\n",
    "                                     pre_dispatch=8,n_iter = 30).fit(X, y_boolean)\n",
    "\n",
    "    t1 = time.localtime()\n",
    "    print('Done at' , time.strftime(\"%H:%M:%S\", t1))\n",
    "with open('dump.p', 'wb') as f:\n",
    "    pickle.dump({'in' : grid_param ,\n",
    "                 'out': grid_search}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_fit_time\n",
      "\t[690.02867977  37.32141058 369.89045056 137.35754371 456.18932152\n",
      " 184.8494792  236.27336701 402.69934948 289.6661249  756.45097788\n",
      " 268.18874661 187.31101513  66.68613791 175.29305712  90.3015159 ]\n",
      "std_fit_time\n",
      "\t[ 5.4261045   4.62307497 50.23601666  1.57604928  8.96949775  3.51713334\n",
      "  4.20336944  3.99793306 11.44582785 18.09002713  7.72037476  1.22601322\n",
      "  0.6180658   4.00822898 15.02676939]\n",
      "mean_score_time\n",
      "\t[7.50773851 5.12932785 6.34752345 4.35355218 5.90589515 6.3094004\n",
      " 4.53880405 6.44200897 3.91615399 5.21079381 9.53086813 6.47203175\n",
      " 6.27580039 4.19172374 2.21245686]\n",
      "std_score_time\n",
      "\t[0.21851615 0.71846843 0.46596361 0.10303288 0.20755637 0.20009386\n",
      " 0.14472093 0.11510817 0.48230765 0.36749729 0.60210629 0.21899587\n",
      " 0.3289914  0.46973708 0.67657892]\n",
      "param_tfidf__token_pattern\n",
      "\t['\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b[a-zA-Z]{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b'\n",
      " '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b' '\\\\b\\\\w{3,}\\\\b'\n",
      " '\\\\b[a-zA-Z]{3,}\\\\b']\n",
      "param_tfidf__strip_accents\n",
      "\t[None None 'unicode' 'unicode' 'unicode' 'unicode' 'unicode' None\n",
      " 'unicode' None None 'unicode' None None None]\n",
      "param_tfidf__norm\n",
      "\t['l2' 'l2' 'l2' 'l1' 'l2' 'l1' 'l1' 'l1' 'l1' 'l1' 'l1' 'l1' 'l2' 'l2'\n",
      " 'l2']\n",
      "param_tfidf__ngram_range\n",
      "\t[(1, 2) (1, 1) (1, 2) (2, 2) (1, 2) (1, 2) (2, 2) (1, 2) (1, 1) (2, 2)\n",
      " (1, 3) (1, 2) (1, 2) (2, 2) (1, 1)]\n",
      "param_tfidf__min_df\n",
      "\t[4 3 3 4 4 4 3 3 4 4 3 4 4 3 4]\n",
      "param_tfidf__max_features\n",
      "\t[None 15000 20000 30000 15000 15000 20000 25000 None None 20000 15000\n",
      " 20000 15000 20000]\n",
      "param_tfidf__max_df\n",
      "\t[0.75 0.25 0.25 0.25 0.25 0.5 0.75 0.75 0.5 0.75 0.25 0.5 0.25 0.25 0.5]\n",
      "param_norm__norm\n",
      "\t['l1' 'l2' 'l1' 'l2' 'l2' 'max' 'max' 'l1' 'l1' 'l2' 'l1' 'max' 'l2' 'l2'\n",
      " 'max']\n",
      "param_clf__solver\n",
      "\t['saga' 'sag' 'saga' 'saga' 'saga' 'saga' 'sag' 'saga' 'sag' 'sag' 'sag'\n",
      " 'saga' 'sag' 'saga' 'saga']\n",
      "param_clf__Cs\n",
      "\t[3 1 30 10 100 3 100 30 100 100 3 3 1 30 3]\n",
      "params\n",
      "\t[{'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.75, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 3, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 1}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 30000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 10}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.75, 'norm__norm': 'max', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 25000, 'tfidf__max_df': 0.75, 'norm__norm': 'l1', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.5, 'norm__norm': 'l1', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 4, 'tfidf__max_features': None, 'tfidf__max_df': 0.75, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 100}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 3), 'tfidf__min_df': 3, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l1', 'clf__solver': 'sag', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': 'unicode', 'tfidf__norm': 'l1', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 4, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'sag', 'clf__Cs': 1}, {'tfidf__token_pattern': '\\\\b\\\\w{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (2, 2), 'tfidf__min_df': 3, 'tfidf__max_features': 15000, 'tfidf__max_df': 0.25, 'norm__norm': 'l2', 'clf__solver': 'saga', 'clf__Cs': 30}, {'tfidf__token_pattern': '\\\\b[a-zA-Z]{3,}\\\\b', 'tfidf__strip_accents': None, 'tfidf__norm': 'l2', 'tfidf__ngram_range': (1, 1), 'tfidf__min_df': 4, 'tfidf__max_features': 20000, 'tfidf__max_df': 0.5, 'norm__norm': 'max', 'clf__solver': 'saga', 'clf__Cs': 3}]\n",
      "split0_test_score\n",
      "\t[0.89485526 0.50524974 0.88870556 0.87288136 0.88615569 0.88255587\n",
      " 0.86703165 0.89223039 0.87490625 0.88233088 0.88750562 0.88225589\n",
      " 0.50524974 0.86440678 0.8720564 ]\n",
      "split1_test_score\n",
      "\t[0.9038476  0.50521263 0.90152254 0.87489687 0.89739743 0.89289732\n",
      " 0.8679967  0.90197255 0.88779719 0.8841221  0.89859746 0.89274732\n",
      " 0.50521263 0.86462162 0.88712218]\n",
      "split2_test_score\n",
      "\t[0.89949749 0.50528763 0.8961974  0.87924698 0.89552239 0.88839721\n",
      " 0.87114678 0.89724743 0.879997   0.88824721 0.89522238 0.88824721\n",
      " 0.50528763 0.8680717  0.880147  ]\n",
      "mean_test_score\n",
      "\t[0.89940011 0.50525    0.89547517 0.87567507 0.89302517 0.88795013\n",
      " 0.86872504 0.89715012 0.88090015 0.88490006 0.89377516 0.88775014\n",
      " 0.50525    0.86570003 0.87977519]\n",
      "std_test_score\n",
      "\t[3.67175264e-03 3.06199498e-05 5.25737068e-03 2.65637467e-03\n",
      " 4.91740188e-03 4.23369856e-03 1.75716367e-03 3.97781566e-03\n",
      " 5.30131084e-03 2.47718070e-03 4.64241837e-03 4.29750638e-03\n",
      " 3.06199498e-05 1.67931546e-03 6.15619584e-03]\n",
      "rank_test_score\n",
      "\t[ 1 14  3 11  5  6 12  2  9  8  4  7 14 13 10]\n",
      "0\n",
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "             print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "pretty(grid_search.cv_results_, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_index_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Em vez de correr o codigo em cima, podemos fazer import do ficheiro dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory='C:\\\\Users\\\\pedro\\\\AppData\\\\Local\\\\Temp\\\\sklearn_graph_cache_mwl12peq',\n",
      "         steps=[('tfidf',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.75, max_features=None,\n",
      "                                 min_df=4, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 st...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('norm', Normalizer(copy=True, norm='l1')),\n",
      "                ('clf',\n",
      "                 LogisticRegressionCV(Cs=3, class_weight=None, cv=None,\n",
      "                                      dual=False, fit_intercept=True,\n",
      "                                      intercept_scaling=1.0, l1_ratios=None,\n",
      "                                      max_iter=1000, multi_class='auto',\n",
      "                                      n_jobs=None, penalty='l2',\n",
      "                                      random_state=None, refit=True,\n",
      "                                      scoring=None, solver='saga', tol=0.001,\n",
      "                                      verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "with open('dump.p' ,'rb') as f:\n",
    "    global grid_param, grid_search\n",
    "    temp = pickle.load(f)\n",
    "    grid_param = temp['in']\n",
    "    grid_search = temp['out']\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TFIDF optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf_dump.p', 'wb') as f:\n",
    "    tfidf = TfidfVectorizer(max_df=0.75 , min_df=4,ngram_range=(1, 2), norm='l2', token_pattern=r'\\b\\w{3,}\\b',).fit(X)\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao booleana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = text2vector(X, targets=y_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "{1: array([[0.50525 , 0.888   , 0.89675 ],\n",
      "       [0.50525 , 0.893625, 0.90275 ],\n",
      "       [0.50525 , 0.898875, 0.904875],\n",
      "       [0.50525 , 0.903875, 0.913375],\n",
      "       [0.50525 , 0.895625, 0.909125]])}\n"
     ]
    }
   ],
   "source": [
    "##tfidf = TfidfVectorizer(min_df = 3, token_pattern=r'\\b\\w{3,}\\b', norm = 'l2' , ngram_range=(1,2)).fit(Docs)\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(vector,y_boolean)\n",
    "print(dl.score(vector, y_boolean))\n",
    "print(dl.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9085\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vector, y_boolean, test_size=1/4)\n",
    "\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(x_train, y_train)\n",
    "print(dl.score(x_train, y_train))\n",
    "print(dl.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificacao multi-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803\n",
      "0.4529\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(vector, y, test_size=1/4)\n",
    "\n",
    "dl = LogisticRegressionCV(max_iter = 1000, Cs=3, tol = 1e-3, cv=5, penalty='l2', solver='saga').fit(x_train, y_train)\n",
    "print(dl.score(x_train, y_train))\n",
    "print(dl.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y1,y2=train_test_split(vector,y,test_size=1/5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=LinearSVC(dual=False,C=1).fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ye=svm.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho y:  (40000,)\n",
      "tamanho ye:  (8000,)\n",
      "N erros(teste): 4813\n"
     ]
    }
   ],
   "source": [
    "print(\"tamanho y: \",y.shape)\n",
    "print(\"tamanho ye: \",ye.shape)\n",
    "print('N erros(teste):',np.sum(y2!=ye))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score train:  0.85334375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score train: \", svm.score(X1,y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Score test:  0.398375\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear SVC Score test: \", svm.score(X2,y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas condições de teste não são as mais adecuadas para o conjunto de dados pois como podemos visualizar no teste acima, os valores de score para o conjunto de train e teste são bastante díspares,sendo o de train bastante bom ao contrário do de test que não é muito bom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposição em componentes principais\n",
    "\n",
    "Temos 4000 documentos, com 240000 tokens, logo para reduzir a complixidade usamos LDA. A LDA remove os componentes com pouco relevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 210238)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não repetir, demora muito, tipo umas três horas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=3800).fit(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30966853623086243"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_dump.p' ,'wb') as f:\n",
    "    pickle.dump(pca , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [10,20,30,50,100,200,300,500]\n",
    "\n",
    "for n_comps in list:\n",
    "    pca = TruncatedSVD(n_components=n_comps, n_iter=2).fit(vector)\n",
    "    print(f'Num Componentes {n_comps : >10} ||  Variance {pca.explained_variance_ratio_.sum() : .10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_dump.p' , 'rb') as f:\n",
    "    global pca\n",
    "    pca = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Escolher o classificador\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "\n",
    "### Pipeline\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "### Clustering\n",
    "- https://medium.com/hanman/data-clustering-what-type-of-movies-are-in-the-imdb-top-250-7ef59372a93b\n",
    "\n",
    "### Decomposição PCA/LSA\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD\n",
    "- https://www.datascienceassn.org/sites/default/files/users/user1/lsa_presentation_final.pdf\n",
    "- https://towardsdatascience.com/latent-semantic-analysis-intuition-math-implementation-a194aff870f8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
