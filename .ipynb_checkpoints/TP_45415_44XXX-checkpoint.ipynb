{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n",
    "\n",
    "## Introdução\n",
    "\n",
    "### Outras Cenas tenho preguiça de escrever, nomeadamente introdução teórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o porter e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'porter', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, PorterStemmer())\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='porter', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs):\n",
    "    X = tfidf.transform(Docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  1 10  1  2  8  8  1 10  2]\n",
      "[1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# [f(x) if condition else g(x) for x in sequence]\n",
    "y_boolean = [0 if val<5 else 1 for val in y]\n",
    "\n",
    "print(y[:10])\n",
    "print(y_boolean[:10])\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(Docs,y_boolean,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificacao Booleana Uni Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar classificacao com stopwords e sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 35751\n",
      "Train score 0.961833\n",
      "Test score  0.8949\n",
      "Confusion Matrix\n",
      " [[4474  570]\n",
      " [ 481 4475]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', stop_words='english').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(X1,y_train)\n",
    "y2e = dl.predict(X2)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, y2e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 36027\n",
      "Train score 0.9591\n",
      "Test score  0.9005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a preservação das stopwords, o classificador obtem melhores resultados, esta diferença será mais visivel com a presença de N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incluir palavras ou palavras e numeros\n",
    "-- Descricao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 36027\n",
      "Train score 0.9591\n",
      "Test score  0.9005\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 35629\n",
      "Train score 0.959167\n",
      "Test score  0.9004\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o numero de tokens é semelhante, com cerca de 400 tokens de diferença, e que os resultados são semelhantes pelo que incluir numeros tem pouca relevancia neste classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparacao com variacao da min_df\n",
    "\n",
    "A min_df corresponde ao numero de repeticoes que tem de existir entre os documentos para que a palavra seja adicionada como token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------- \n",
      "V 1\n",
      "train 0.962867\n",
      "test 0.9014\n",
      "diff 0.061467\n",
      "num tokens 77704\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 2\n",
      "train 0.960633\n",
      "test 0.9019\n",
      "diff 0.058733\n",
      "num tokens 45514\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 3\n",
      "train 0.9591\n",
      "test 0.9005\n",
      "diff 0.0586\n",
      "num tokens 36027\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 4\n",
      "train 0.9588\n",
      "test 0.9008\n",
      "diff 0.058\n",
      "num tokens 30833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 5\n",
      "train 0.957833\n",
      "test 0.9007\n",
      "diff 0.057133\n",
      "num tokens 27282\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 6\n",
      "train 0.9567\n",
      "test 0.9\n",
      "diff 0.0567\n",
      "num tokens 24672\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 7\n",
      "train 0.955733\n",
      "test 0.899\n",
      "diff 0.056733\n",
      "num tokens 22616\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 8\n",
      "train 0.9552\n",
      "test 0.8986\n",
      "diff 0.0566\n",
      "num tokens 21042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 9\n",
      "train 0.954433\n",
      "test 0.898\n",
      "diff 0.056433\n",
      "num tokens 19653\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vals = np.arange(1, 10 , 1)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "\n",
    "for v in vals:\n",
    "    tfidf = TfidfVectorizer(min_df=v, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "    X1 = text2vector(x_train)\n",
    "    X2 = text2vector(x_test)\n",
    "    \n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nV', round(v,2))\n",
    "    train_score =round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('num tokens', len(tfidf.get_feature_names()))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aumentar o min_df vemos que a frequencia tem pouca relevancia no algoritmo, no entanto valores pequenos levam a uma expansao enorme no numero de tokens considerados, que por sua vez irá gerar um peso de computação maior em pasos adiante. Como equilibrio entre a velocidade de execução e a fiabilidade do algoritmo escolhemos um **valor de 3.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar diversos criterios de regularizacao (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.1\n",
      "train 0.8778\n",
      "test 0.8666\n",
      "diff 0.0112\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.3\n",
      "train 0.903733\n",
      "test 0.8824\n",
      "diff 0.021333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.5\n",
      "train 0.915133\n",
      "test 0.8876\n",
      "diff 0.027533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.7\n",
      "train 0.9229\n",
      "test 0.8918\n",
      "diff 0.0311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.9\n",
      "train 0.928833\n",
      "test 0.8934\n",
      "diff 0.035433\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.1\n",
      "train 0.933667\n",
      "test 0.8951\n",
      "diff 0.038567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.3\n",
      "train 0.9372\n",
      "test 0.8961\n",
      "diff 0.0411\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.5\n",
      "train 0.940667\n",
      "test 0.8967\n",
      "diff 0.043967\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.7\n",
      "train 0.944133\n",
      "test 0.8984\n",
      "diff 0.045733\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.9\n",
      "train 0.946733\n",
      "test 0.8989\n",
      "diff 0.047833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.1\n",
      "train 0.9489\n",
      "test 0.8993\n",
      "diff 0.0496\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.3\n",
      "train 0.951233\n",
      "test 0.8999\n",
      "diff 0.051333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.5\n",
      "train 0.953167\n",
      "test 0.9\n",
      "diff 0.053167\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.7\n",
      "train 0.955033\n",
      "test 0.9002\n",
      "diff 0.054833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.9\n",
      "train 0.956833\n",
      "test 0.9008\n",
      "diff 0.056033\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.1\n",
      "train 0.958167\n",
      "test 0.9006\n",
      "diff 0.057567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.3\n",
      "train 0.959133\n",
      "test 0.9005\n",
      "diff 0.058633\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.5\n",
      "train 0.9608\n",
      "test 0.9005\n",
      "diff 0.0603\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.7\n",
      "train 0.962\n",
      "test 0.9007\n",
      "diff 0.0613\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.9\n",
      "train 0.9632\n",
      "test 0.9006\n",
      "diff 0.0626\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.1\n",
      "train 0.964333\n",
      "test 0.901\n",
      "diff 0.063333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.3\n",
      "train 0.965433\n",
      "test 0.9007\n",
      "diff 0.064733\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.5\n",
      "train 0.966667\n",
      "test 0.9004\n",
      "diff 0.066267\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.7\n",
      "train 0.967667\n",
      "test 0.9002\n",
      "diff 0.067467\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.9\n",
      "train 0.968567\n",
      "test 0.9002\n",
      "diff 0.068367\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "vals = np.arange(0.1, 5 , 0.2)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for c in vals:\n",
    "    dl = LogisticRegression(max_iter = 1000, C=c, tol = 1e-3)\n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nC', round(c,2))\n",
    "    train_score = round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score  = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('-'*100)\n",
    "\n",
    "diff = np.array(train_scores) - np.array(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901\n",
      "Melhor C =  20\n"
     ]
    }
   ],
   "source": [
    "print(np.max(test_scores))\n",
    "print('Melhor C = ', np.argmax(test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um C mais pequeno, a diferença entre o train e test é menor mas os resultados são piores.\n",
    "\n",
    "O C que obteve os melhores resultados com menos erros foi o 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar cross validation para melhorar o classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89425 0.894   0.90675 0.901   0.8945  0.908   0.90775 0.902   0.897\n",
      " 0.904  ]\n"
     ]
    }
   ],
   "source": [
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "docsVector = text2vector(Docs)\n",
    "scores = cross_val_score(dl, docsVector, y_boolean, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900925\n",
      "0.908\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean())\n",
    "print(scores.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time  112.16232132911682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lrcv = LogisticRegressionCV(max_iter = 1000, tol=1e-3, cv=10, penalty='l2', n_jobs=-1).fit(X1, y_train)\n",
    "y2e = lrcv.predict(X2)\n",
    "\n",
    "print('Exec time ',time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4502  453]\n",
      " [ 542 4503]]\n",
      "0.9557333333333333\n",
      "0.9005\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y2e, y_test))\n",
    "print(lrcv.score(X1, y_train))\n",
    "print(lrcv.score(X2, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LogisticRegressionCV in module sklearn.linear_model._logistic object:\n",
      "\n",
      "class LogisticRegressionCV(LogisticRegression, sklearn.base.BaseEstimator, sklearn.linear_model._base.LinearClassifierMixin)\n",
      " |  LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      " |  \n",
      " |  Logistic Regression CV (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  See glossary entry for :term:`cross-validation estimator`.\n",
      " |  \n",
      " |  This class implements logistic regression using liblinear, newton-cg, sag\n",
      " |  of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\n",
      " |  regularization with primal formulation. The liblinear solver supports both\n",
      " |  L1 and L2 regularization, with a dual formulation only for the L2 penalty.\n",
      " |  Elastic-Net penalty is only supported by the saga solver.\n",
      " |  \n",
      " |  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter\n",
      " |  is selected by the cross-validator\n",
      " |  :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed\n",
      " |  using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |  solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  Cs : int or list of floats, default=10\n",
      " |      Each of the values in Cs describes the inverse of regularization\n",
      " |      strength. If Cs is as an int, then a grid of Cs values are chosen\n",
      " |      in a logarithmic scale between 1e-4 and 1e4.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  cv : int or cross-validation generator, default=None\n",
      " |      The default cross-validation generator used is Stratified K-Folds.\n",
      " |      If an integer is provided, then it is the number of folds used.\n",
      " |      See the module :mod:`sklearn.model_selection` module for the\n",
      " |      list of possible cross-validation objects.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  penalty : {'l1', 'l2', 'elasticnet'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver.\n",
      " |  \n",
      " |  scoring : str or callable, default=None\n",
      " |      A string (see model evaluation documentation) or\n",
      " |      a scorer callable object / function with signature\n",
      " |      ``scorer(estimator, X, y)``. For a list of scoring functions\n",
      " |      that can be used, look at :mod:`sklearn.metrics`. The\n",
      " |      default scoring option used is 'accuracy'.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n",
      " |        'liblinear' and 'saga' handle L1 penalty.\n",
      " |      - 'liblinear' might be slower in LogisticRegressionCV because it does\n",
      " |        not handle warm-starting.\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can preprocess the data\n",
      " |      with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations of the optimization algorithm.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         class_weight == 'balanced'\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used during the cross-validation loop.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n",
      " |      positive number for verbosity.\n",
      " |  \n",
      " |  refit : bool, default=True\n",
      " |      If set to True, the scores are averaged across all folds, and the\n",
      " |      coefs and the C that corresponds to the best score is taken, and a\n",
      " |      final refit is done using these parameters.\n",
      " |      Otherwise the coefs, intercepts and C that correspond to the\n",
      " |      best scores across folds are averaged.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  multi_class : {'auto, 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.\n",
      " |      Note that this only applies to the solver and not the cross-validation\n",
      " |      generator. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  l1_ratios : list of float, default=None\n",
      " |      The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n",
      " |      Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n",
      " |      using ``penalty='l2'``, while 1 is equivalent to using\n",
      " |      ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n",
      " |      of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem\n",
      " |      is binary.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape(1,) when the problem is binary.\n",
      " |  \n",
      " |  Cs_ : ndarray of shape (n_cs)\n",
      " |      Array of C i.e. inverse of regularization parameter values used\n",
      " |      for cross-validation.\n",
      " |  \n",
      " |  l1_ratios_ : ndarray of shape (n_l1_ratios)\n",
      " |      Array of l1_ratios used for cross-validation. If no l1_ratio is used\n",
      " |      (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n",
      " |  \n",
      " |  coefs_paths_ : ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)\n",
      " |      dict with classes as the keys, and the path of coefficients obtained\n",
      " |      during cross-validating across each fold and then across each Cs\n",
      " |      after doing an OvR for the corresponding class as values.\n",
      " |      If the 'multi_class' option is set to 'multinomial', then\n",
      " |      the coefs_paths are the coefficients corresponding to each class.\n",
      " |      Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n",
      " |      ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n",
      " |      intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n",
      " |      ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n",
      " |      ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n",
      " |  \n",
      " |  scores_ : dict\n",
      " |      dict with classes as the keys, and the values as the\n",
      " |      grid of scores obtained during cross-validating each fold, after doing\n",
      " |      an OvR for the corresponding class. If the 'multi_class' option\n",
      " |      given is 'multinomial' then the same scores are repeated across\n",
      " |      all classes, since this is the multinomial class. Each dict value\n",
      " |      has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n",
      " |      ``penalty='elasticnet'``.\n",
      " |  \n",
      " |  C_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      " |      Array of C that maps to the best scores across every class. If refit is\n",
      " |      set to False, then for each class, the best C is the average of the\n",
      " |      C's that correspond to the best scores for each fold.\n",
      " |      `C_` is of shape(n_classes,) when the problem is binary.\n",
      " |  \n",
      " |  l1_ratio_ : ndarray of shape (n_classes,) or (n_classes - 1,)\n",
      " |      Array of l1_ratio that maps to the best scores across every class. If\n",
      " |      refit is set to False, then for each class, the best l1_ratio is the\n",
      " |      average of the l1_ratio's that correspond to the best scores for each\n",
      " |      fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n",
      " |      Actual number of iterations for all classes, folds and Cs.\n",
      " |      In the binary or multinomial cases, the first dimension is equal to 1.\n",
      " |      If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n",
      " |      n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n",
      " |  \n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegressionCV\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :]).shape\n",
      " |  (2, 3)\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.98...\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  LogisticRegression\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegressionCV\n",
      " |      LogisticRegression\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the score using the `scoring` option on the given\n",
      " |      test data and labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Score of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LogisticRegression:\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is the signed distance of that\n",
      " |      sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "help(lrcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizar o GridSearchCV \n",
    "\n",
    "Vamos utilizar o modulo GridSearchCV para determinar quais os melhores parametros para o nosso classificador, tudo o que está em cima pode ser reduzido ao seguinte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificacao booleana, N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for low in range(10):\n",
    "    new_line = []\n",
    "    for high in range(10):        \n",
    "        tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', ngram_range=(low,high)).fit(x_train)\n",
    "        X1 = text2vector(x_train)\n",
    "        X2 = text2vector(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
