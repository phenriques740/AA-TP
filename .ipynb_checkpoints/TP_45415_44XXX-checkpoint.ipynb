{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n",
    "\n",
    "## Introdução\n",
    "\n",
    "### Outras Cenas tenho preguiça de escrever, nomeadamente introdução teórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.grid_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-67abf471d763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLancasterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.grid_search'"
     ]
    }
   ],
   "source": [
    "import re, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o porter e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'porter', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, PorterStemmer())\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='porter', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs):\n",
    "    X = tfidf.transform(Docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  1 10  1  2  8  8  1 10  2]\n",
      "[1, 0, 1, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# [f(x) if condition else g(x) for x in sequence]\n",
    "y_boolean = [0 if val<4 else 1 for val in y]\n",
    "\n",
    "print(y[:10])\n",
    "print(y_boolean[:10])\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(Docs,y_boolean,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificacao Booleana Uni Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar classificacao com stopwords e sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 35740\n",
      "Train score 0.935667\n",
      "Test score  0.8484\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', stop_words='english').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 36016\n",
      "Train score 0.9298\n",
      "Test score  0.8529\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a preservação das stopwords, o classificador obtem melhores resultados, esta diferença será mais visivel com a presença de N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incluir palavras ou palavras e numeros\n",
    "-- Descricao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 36016\n",
      "Train score 0.9298\n",
      "Test score  0.8529\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 35623\n",
      "Train score 0.929767\n",
      "Test score  0.8528\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o numero de tokens é semelhante, com cerca de 400 tokens de diferença, e que os resultados são semelhantes pelo que incluir numeros tem pouca relevancia neste classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparacao com variacao da min_df\n",
    "\n",
    "A min_df corresponde ao numero de repeticoes que tem de existir entre os documentos para que a palavra seja adicionada como token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------- \n",
      "V 1\n",
      "train 0.935933\n",
      "test 0.8503\n",
      "diff 0.085633\n",
      "num tokens 77726\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 2\n",
      "train 0.932267\n",
      "test 0.85\n",
      "diff 0.082267\n",
      "num tokens 45585\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 3\n",
      "train 0.931067\n",
      "test 0.8502\n",
      "diff 0.080867\n",
      "num tokens 36052\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 4\n",
      "train 0.929667\n",
      "test 0.85\n",
      "diff 0.079667\n",
      "num tokens 30803\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 5\n",
      "train 0.928467\n",
      "test 0.8508\n",
      "diff 0.077667\n",
      "num tokens 27273\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 6\n",
      "train 0.9272\n",
      "test 0.8497\n",
      "diff 0.0775\n",
      "num tokens 24686\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 7\n",
      "train 0.926067\n",
      "test 0.8505\n",
      "diff 0.075567\n",
      "num tokens 22620\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 8\n",
      "train 0.9245\n",
      "test 0.8498\n",
      "diff 0.0747\n",
      "num tokens 21005\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "V 9\n",
      "train 0.924133\n",
      "test 0.8497\n",
      "diff 0.074433\n",
      "num tokens 19638\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vals = np.arange(1, 10 , 1)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "\n",
    "for v in vals:\n",
    "    tfidf = TfidfVectorizer(min_df=v, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "    X1 = text2vector(x_train)\n",
    "    X2 = text2vector(x_test)\n",
    "    \n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nV', round(v,2))\n",
    "    train_score =round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('num tokens', len(tfidf.get_feature_names()))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aumentar o min_df vemos que a frequencia tem pouca relevancia no algoritmo, no entanto valores pequenos levam a uma expansao enorme no numero de tokens considerados, que por sua vez irá gerar um peso de computação maior em pasos adiante. Como equilibrio entre a velocidade de execução e a fiabilidade do algoritmo escolhemos um **valor de 3.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar diversos criterios de regularizacao (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "vals = np.arange(0.1, 5 , 0.2)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for c in vals:\n",
    "    dl = LogisticRegression(max_iter = 1000, C=c, tol = 1e-3)\n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nC', round(c,2))\n",
    "    train_score = round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score  = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('-'*100)\n",
    "\n",
    "diff = np.array(train_scores) - np.array(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um C mais pequeno, a diferença entre o train e test é menor mas os resultados são piores.\n",
    "\n",
    "O C que obteve os melhores resultados com menos erros foi o 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificacao booleana, N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for low in range(10):\n",
    "    new_line = []\n",
    "    for high in range(10):        \n",
    "        tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', ngram_range=(low,high)).fit(x_train)\n",
    "        X1 = text2vector(x_train)\n",
    "        X2 = text2vector(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module sklearn.feature_extraction.text:\n",
      "\n",
      "fit(raw_documents, y=None) method of sklearn.feature_extraction.text.TfidfVectorizer instance\n",
      "    Learn vocabulary and idf from training set.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    raw_documents : iterable\n",
      "        An iterable which yields either str, unicode or file objects.\n",
      "    y : None\n",
      "        This parameter is not needed to compute tfidf.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    self : object\n",
      "        Fitted vectorizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tfidf.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
