{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático Aprendizagem Automática\n",
    "\n",
    "## Introdução\n",
    "\n",
    "### Outras Cenas tenho preguiça de escrever, nomeadamente introdução teórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pickle, time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import cross_val_score , GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "#Possiveis de serem usadas : numpy, scipy, matplotlib, sklearn, nltk, re e opencv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('imdbCriticas.p', 'rb') as f:\n",
    "    global D, Docs, y\n",
    "    D = pickle.load(f)\n",
    "    Docs = D.data\n",
    "    y = D.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos este metodo para préprocessar os dados de texto, e reduzir as palavras tendo em conta os erros de ortografia\n",
    "\n",
    "O stemmer por defeito é o porter e se o argumento não corresponder a nenhum outro, este é utilizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessDoc(Doc, stemmer = 'snowball', decode = False):\n",
    "    stem = {\n",
    "        'porter'   : PorterStemmer(),\n",
    "        'snowball' : SnowballStemmer('english'),\n",
    "        'lancaster': LancasterStemmer()\n",
    "    }\n",
    "    stemFunc = stem.get(stemmer, SnowballStemmer('english'))\n",
    "    if(decode):\n",
    "        Doc = Doc.decode('UTF-8')\n",
    "    Doc = Doc.replace('<br />', ' ')\n",
    "    Doc = re.sub(r'[^a-zA-Z\\u00C0\\u00FF]+', ' ', Doc)\n",
    "    Doc = ' '.join([stemFunc.stem(w) for w in Doc.split()])\n",
    "    return Doc\n",
    "\n",
    "def preProcessDocs(Docs, stemmer='snowball', decode = False):\n",
    "    return [preProcessDoc(doc, stemmer, decode) for doc in Docs]\n",
    "\n",
    "def text2vector(Docs, preProcess = False, stemmer='snowball', decode=False):\n",
    "    if(preProcess):\n",
    "        Docs = preProcessDocs(Docs, stemmer=stemmer, decode=decode)\n",
    "    \n",
    "    global tfidf\n",
    "    if(tfidf is None):\n",
    "        tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', ngram_range=(1,2), norm = 'l2').fit(preProcessDocs(Docs))\n",
    "    X = tfidf.transform(Docs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_boolean = [0 if val<5 else 1 for val in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparar os stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs, stemmer='porter')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "tokens = tfidf.get_feature_names()\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tokens))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs, stemmer='snowball')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "tokens = tfidf.get_feature_names()\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tokens))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs, stemmer='lancaster')\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(X)\n",
    "tokens = tfidf.get_feature_names()\n",
    "vector = text2vector(X)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(vector,y_boolean)\n",
    "print('Token len' , len(tokens))\n",
    "print(dl.score(vector, y_boolean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P 28315.486105602708\n",
      "S 27919.818056804357\n"
     ]
    }
   ],
   "source": [
    "print('P' ,26773/0.945525)\n",
    "print('S' ,26394/0.94535)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possivel ver que o lancaster é o que reduz ao maximo a quantidade de tokens, no entanto vamos optar por utilizar o Snowball pois precisava de uma quantidade de tokens menores para atingir os teoricos 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificacao Booleana Uni Gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)\n",
    "x_train,x_test, y_train, y_test = train_test_split(X,y_boolean,test_size=0.25)   \n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', ngram_range=(1,2), norm = 'l2').fit(X, y_boolean)\n",
    "x_train = text2vector(x_train)\n",
    "x_test = text2vector(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos a cross validation para ter resultados mais fiaveis\n",
    "\n",
    "Copiado da documentação\n",
    "\n",
    "\n",
    "\"For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n",
    "\n",
    "For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n",
    "\n",
    "‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n",
    "\n",
    "‘liblinear’ might be slower in LogisticRegressionCV because it does not handle warm-starting.\"\n",
    "\n",
    "liblinear - nao usar, \n",
    "\n",
    "Usar pipeline para fazer tudo o que está abaixo com parametros. Umpa procura exaustiva, com todos os parametros especificados ai, gera $4.3*10^8$ fits possiveis. Pelo que se demorar 1 minuto entre cada fit vamos estar aqui 30000 dias ou 300 anos.\n",
    "\n",
    "**Descomentar linhas na grelha, leva a melhores estimativas, mas demora muito mais tempo.** \n",
    "\n",
    "**Se quiseres faz import das cenas do ficheiro pickle em vez de correr isto** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "pipeline =Pipeline([\n",
    "    ('tfidf' , TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', ngram_range=(1,2), norm = 'l2')),\n",
    "    ('clf' , LogisticRegression(max_iter = 1000, solver='saga', tol=1e-5))\n",
    "])\n",
    "\n",
    "grid_param ={\n",
    "    #'tfidf__strip_accents' :[None, 'unicode'],\n",
    "    #'tfidf__stop_words' : [None, 'english'],\n",
    "    #'tfidf__token_pattern' : [r'\\b\\w{3,}\\b', r'\\b[a-zA-Z]{3,}\\b'],\n",
    "    #'tfidf__min_df' : np.arange(1, 5, 1),\n",
    "    #'tfidf__min_df' : [3,4,5],\n",
    "    #'tfidf__min_df' : [3],\n",
    "    #'tfidf__ngram_range' : [(i,j) for i in range(1,5) for j in range(1,5)],\n",
    "    #'tfidf__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    #'tfidf__norm' : ['l1', 'l2'],\n",
    "    \n",
    "    #'clf__C' : np.linspace(5,10,10),\n",
    "    #'clf__C' :[0.1, 3.3, 10], \n",
    "    #'clf__C' : [3.3],\n",
    "    #'clf__solver' : ['sag', 'saga'],\n",
    "    #'clf__tol' : (1e-3, 1e-4, 1e-5)\n",
    "}\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.22.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('dump.p' ,'rb') as f:\n",
    "    global grid_param, grid_search\n",
    "    temp = pickle.load(f)\n",
    "    grid_param = temp['param']\n",
    "    grid_search = temp['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos a usar cross validation com 5 folds, não é preciso dividir em treino e teste pois o algoritmo faz isso sozinho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 24.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exec time  1516.706059217453\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "grid_search = GridSearchCV(pipeline, grid_param, scoring='accuracy', n_jobs=-1, verbose=2, cv=3).fit(X, y_boolean)\n",
    "print('Exec time ',time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf', TfidfVectorizer(min_df=3, ngram_range=(1, 2))),\n",
       "  ('clf', LogisticRegression(C=3.3, max_iter=1000, solver='saga'))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(min_df=3, ngram_range=(1, 2)),\n",
       " 'clf': LogisticRegression(C=3.3, max_iter=1000, solver='saga'),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 3,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'clf__C': 3.3,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__dual': False,\n",
       " 'clf__fit_intercept': True,\n",
       " 'clf__intercept_scaling': 1,\n",
       " 'clf__l1_ratio': None,\n",
       " 'clf__max_iter': 1000,\n",
       " 'clf__multi_class': 'auto',\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__penalty': 'l2',\n",
       " 'clf__random_state': None,\n",
       " 'clf__solver': 'saga',\n",
       " 'clf__tol': 0.0001,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dump3.p', 'wb') as f:\n",
    "    pickle.dump({'param' : grid_param , 'out' :grid_search}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.904950\n",
      "Best parameters set:\n",
      "\tclf__C: 10.0\n",
      "\tclf__tol: 0.0001\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.6f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(grid_param.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.984167\n",
      "Test score  0.9055\n",
      "Confusion Matrix\n",
      " [[4613  523]\n",
      " [ 422 4442]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w{3,}\\b', ngram_range=(1,2), norm = 'l2').fit(X, y_boolean)\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-4).fit(x_train,y_train)\n",
    "y2e = dl.predict(x_test)\n",
    "\n",
    "print('Train score' , round(dl.score(x_train,y_train),6))\n",
    "print('Test score ' , round(dl.score(x_test,y_test), 6))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, y2e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar classificacao com stopwords e sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 23377\n",
      "Train score 0.950967\n",
      "Test score  0.8883\n",
      "Confusion Matrix\n",
      " [[4440  636]\n",
      " [ 481 4443]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', stop_words='english').fit(x_train, y_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(X1,y_train)\n",
    "y2e = dl.predict(X2)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, y2e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token len 23568\n",
      "Train score 0.950533\n",
      "Test score  0.8916\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train, y_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3).fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a preservação das stopwords, o classificador obtem melhores resultados, esta diferença será mais visivel com a presença de N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incluir palavras ou palavras e numeros\n",
    "-- Descricao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train, y_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b[a-zA-Z]{3,}\\b').fit(x_train, y_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "print('Token len' , len(tokens))\n",
    "\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "dl.fit(X1,y_train)\n",
    "\n",
    "print('Train score' , round(dl.score(X1,y_train),6))\n",
    "print('Test score ' , round(dl.score(X2,y_test), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o numero de tokens é semelhante, com cerca de 400 tokens de diferença, e que os resultados são semelhantes pelo que incluir numeros tem pouca relevancia neste classificador, no entanto vamos remover os numeros pois os resultados foram semelhantes e reduz a complexidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparacao com variacao da min_df\n",
    "\n",
    "A min_df corresponde ao numero de repeticoes que tem de existir entre os documentos para que a palavra seja adicionada como token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vals = np.arange(1, 10 , 1)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "dl = LogisticRegression(max_iter = 1000, C=3.3, tol = 1e-3)\n",
    "\n",
    "for v in vals:\n",
    "    tfidf = TfidfVectorizer(min_df=v, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "    X1 = text2vector(x_train)\n",
    "    X2 = text2vector(x_test)\n",
    "    \n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nV', round(v,2))\n",
    "    train_score =round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('num tokens', len(tfidf.get_feature_names()))\n",
    "    print('-'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao aumentar o min_df vemos que a frequencia tem pouca relevancia no algoritmo, no entanto valores pequenos levam a uma expansao enorme no numero de tokens considerados, que por sua vez irá gerar um peso de computação maior em pasos adiante. Como equilibrio entre a velocidade de execução e a fiabilidade do algoritmo escolhemos um **valor de 3.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparar diversos criterios de regularizacao (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.1\n",
      "train 0.878367\n",
      "test 0.8637\n",
      "diff 0.014667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.3\n",
      "train 0.900433\n",
      "test 0.8784\n",
      "diff 0.022033\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.5\n",
      "train 0.9108\n",
      "test 0.8832\n",
      "diff 0.0276\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.7\n",
      "train 0.917633\n",
      "test 0.8863\n",
      "diff 0.031333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 0.9\n",
      "train 0.922467\n",
      "test 0.8871\n",
      "diff 0.035367\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.1\n",
      "train 0.9267\n",
      "test 0.8893\n",
      "diff 0.0374\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.3\n",
      "train 0.9302\n",
      "test 0.89\n",
      "diff 0.0402\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.5\n",
      "train 0.9334\n",
      "test 0.8899\n",
      "diff 0.0435\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.7\n",
      "train 0.936167\n",
      "test 0.8897\n",
      "diff 0.046467\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 1.9\n",
      "train 0.939\n",
      "test 0.8898\n",
      "diff 0.0492\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.1\n",
      "train 0.9411\n",
      "test 0.8899\n",
      "diff 0.0512\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.3\n",
      "train 0.942933\n",
      "test 0.8904\n",
      "diff 0.052533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.5\n",
      "train 0.944633\n",
      "test 0.8909\n",
      "diff 0.053733\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.7\n",
      "train 0.9462\n",
      "test 0.8912\n",
      "diff 0.055\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 2.9\n",
      "train 0.947733\n",
      "test 0.8914\n",
      "diff 0.056333\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.1\n",
      "train 0.9494\n",
      "test 0.8912\n",
      "diff 0.0582\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.3\n",
      "train 0.950533\n",
      "test 0.8916\n",
      "diff 0.058933\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.5\n",
      "train 0.9516\n",
      "test 0.8915\n",
      "diff 0.0601\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.7\n",
      "train 0.952767\n",
      "test 0.8918\n",
      "diff 0.060967\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 3.9\n",
      "train 0.953533\n",
      "test 0.8916\n",
      "diff 0.061933\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.1\n",
      "train 0.954633\n",
      "test 0.8915\n",
      "diff 0.063133\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.3\n",
      "train 0.955567\n",
      "test 0.8915\n",
      "diff 0.064067\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.5\n",
      "train 0.9563\n",
      "test 0.8912\n",
      "diff 0.0651\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.7\n",
      "train 0.957133\n",
      "test 0.8914\n",
      "diff 0.065733\n",
      "----------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------- \n",
      "C 4.9\n",
      "train 0.958067\n",
      "test 0.8917\n",
      "diff 0.066367\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b').fit(x_train)\n",
    "tokens = tfidf.get_feature_names()\n",
    "X1 = text2vector(x_train)\n",
    "X2 = text2vector(x_test)\n",
    "vals = np.arange(0.1, 5 , 0.2)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for c in vals:\n",
    "    dl = LogisticRegression(max_iter = 1000, C=c, tol = 1e-3)\n",
    "    dl.fit(X1,y_train)## Classificacao booleana, uni-gramas\n",
    "    print('-'*100, '\\nC', round(c,2))\n",
    "    train_score = round(dl.score(X1,y_train),6)\n",
    "    train_scores.append(train_score)\n",
    "    print('train' ,train_score)\n",
    "    test_score  = round(dl.score(X2,y_test),6)\n",
    "    test_scores.append(test_score)\n",
    "    print('test' ,test_score)\n",
    "    print('diff' , round(train_score-test_score,6))\n",
    "    print('-'*100)\n",
    "\n",
    "diff = np.array(train_scores) - np.array(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8918\n",
      "Melhor C =  3.7000000000000006\n"
     ]
    }
   ],
   "source": [
    "print(np.max(test_scores))\n",
    "print('Melhor C = ', vals[np.argmax(test_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com um C mais pequeno, a diferença entre o train e test é menor mas os resultados são piores.\n",
    "\n",
    "O C que obteve os melhores resultados com menos erros foi o 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usar clustering\n",
    "\n",
    "Segundo o SKLearn o K-means é o melhor metodo para os dados disponiveis por isso vamos usar.\n",
    "\n",
    "### 1º Modelar os dados usando PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preProcessDocs(Docs)\n",
    "tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', stop_words='english').fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificacao booleana, N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for low in range(10):\n",
    "    new_line = []\n",
    "    for high in range(10):        \n",
    "        tfidf = TfidfVectorizer(min_df=3, token_pattern=r'\\b\\w\\w\\w+\\b', ngram_range=(low,high)).fit(x_train)\n",
    "        X1 = text2vector(x_train)\n",
    "        X2 = text2vector(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia\n",
    "\n",
    "### Geral\n",
    "- Slides Professor\n",
    "\n",
    "### Pré processamento do texto\n",
    "- https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae\n",
    "- https://medium.com/@wenxuan0923/feature-extraction-from-text-using-countvectorizer-tfidfvectorizer-9f74f38f86cc\n",
    "\n",
    "### Escolher o classificador\n",
    "- https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "### Grid Search\n",
    "- https://scikit-learn.org/stable/modules/grid_search.html\n",
    "------------------ VER O Proximo para fazer a estimativa do TFIDF\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "\n",
    "### Pipeline\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline\n",
    "\n",
    "### Clustering\n",
    "- https://medium.com/hanman/data-clustering-what-type-of-movies-are-in-the-imdb-top-250-7ef59372a93b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
